{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eikfzi8ZT_rW"
      },
      "source": [
        "# Local file system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaCkyg5CV5jF"
      },
      "source": [
        "## Uploading files from your local file system\n",
        "\n",
        "`files.upload` returns a dictionary of the files which were uploaded.\n",
        "The dictionary is keyed by the file name and values are the data which were uploaded."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB2tcr0Vdfku",
        "outputId": "38f16636-5c7d-437b-e44a-3d2e44d839b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.manifold\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "# Add some convenience functions to Pandas DataFrame.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "def mask(df, key, function):\n",
        "  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
        "  return df[function(df[key])]\n",
        "\n",
        "def flatten_cols(df):\n",
        "  df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
        "  return df\n",
        "\n",
        "pd.DataFrame.mask = mask\n",
        "pd.DataFrame.flatten_cols = flatten_cols\n",
        "\n",
        "# Install Altair and activate its colab renderer.\n",
        "print(\"Installing Altair...\")\n",
        "import altair as alt\n",
        "alt.data_transformers.enable('default', max_rows=None)\n",
        "alt.renderers.enable('colab')\n",
        "print(\"Done installing Altair.\")\n"
      ],
      "metadata": {
        "id": "C1_-jJCtEaxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading data...\")\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "# Load each data set (users, items, and ratings).\n",
        "users_cols = ['user_id','gender']\n",
        "users = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/user.csv')\n",
        "users=users[0:40000]\n",
        "\n",
        "ratings_cols = ['item_id', 'user_id', 'rating', 'timestamp']\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ratings.csv')\n",
        "ratings=ratings[0:40000]\n",
        "\n",
        "items_cols = ['item_id', 'category', 'brand', \"year\"]\n",
        "items = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/item.csv')\n",
        "items=items[0:40000]\n",
        "\n",
        "All_items_cols = ['user_id','gender','item_id', 'rating', 'timestamp', 'category', 'brand', \"year\"]\n",
        "All_items = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/All_Items.csv')\n",
        "All_items=All_items[0:40000]\n",
        "# Utility to split the data into training and test sets.\n",
        "def split_dataframe(df, holdout_fraction=0.1):\n",
        "  \"\"\"Splits a DataFrame into training and test sets.\n",
        "  Args:\n",
        "    df: a dataframe.\n",
        "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
        "  Returns:\n",
        "    train: dataframe for training\n",
        "    test: dataframe for testing\n",
        "  \"\"\"\n",
        "  test = df.sample(frac=holdout_fraction, replace=False)\n",
        "  train = df[~df.index.isin(test.index)]\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "PcMOJyCWrHP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "All_items.describe(include=[np.object])"
      ],
      "metadata": {
        "id": "2wdM-XcJUB8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.info"
      ],
      "metadata": {
        "id": "E0D4t9mgcgFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Altair visualization code (run this cell)\n",
        "# The following functions are used to generate interactive Altair charts.\n",
        "# We will display histograms of the data, sliced by a given attribute.\n",
        "# Create filters to be used to slice the data.\n",
        "category_filter = alt.selection_multi(fields=[\"category\"])\n",
        "category_chart = alt.Chart().mark_bar().encode(\n",
        "    x=\"count()\",\n",
        "    y=alt.Y(\"category:N\"),\n",
        "    color=alt.condition(\n",
        "        category_filter,\n",
        "        alt.Color(\"category:N\", scale=alt.Scale(scheme='category20')),\n",
        "        alt.value(\"lightgray\")),\n",
        ").properties(width=300, height=300, selection=category_filter)\n",
        "\n",
        "# A function that generates a histogram of filtered data.\n",
        "def filtered_hist(field, label, filter):\n",
        "  \"\"\"Creates a layered chart of histograms.\n",
        "  The first layer (light gray) contains the histogram of the full data, and the\n",
        "  second contains the histogram of the filtered data.\n",
        "  Args:\n",
        "    field: the field for which to generate the histogram.\n",
        "    label: String label of the histogram.\n",
        "    filter: an alt.Selection object to be used to filter the data.\n",
        "  \"\"\"\n",
        "  base = alt.Chart().mark_bar().encode(\n",
        "      x=alt.X(field, bin=alt.Bin(maxbins=10), title=label),\n",
        "      y=\"count()\",\n",
        "  ).properties(\n",
        "      width=300,\n",
        "  )\n",
        "  return alt.layer(\n",
        "      base.transform_filter(filter),\n",
        "      base.encode(color=alt.value('lightgray'), opacity=alt.value(.7)),\n",
        "  ).resolve_scale(y='independent')\n"
      ],
      "metadata": {
        "id": "vqDpWMUhcaQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_ratings = (\n",
        "    ratings\n",
        "    .groupby('item_id', as_index=False)\n",
        "    .agg({'rating': ['count', 'mean']})\n",
        "    .flatten_cols()\n",
        "    .merge(items, on='item_id')\n",
        "    )\n",
        "# Create a chart for the count, and one for the mean.\n",
        "alt.hconcat(\n",
        "    filtered_hist('rating count', 'Number of items', category_filter),\n",
        "    filtered_hist('rating mean', 'mean items rating', category_filter),\n",
        "    category_chart,\n",
        "    data=items_ratings)"
      ],
      "metadata": {
        "id": "A72yHnFrehGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Altair visualization code (run this cell)\n",
        "# The following functions are used to generate interactive Altair charts.\n",
        "# We will display histograms of the data, sliced by a given attribute.\n",
        "# Create filters to be used to slice the data.\n",
        "category_filter = alt.selection_multi(fields=[\"gender\"])\n",
        "category_chart = alt.Chart().mark_bar().encode(\n",
        "    x=\"count()\",\n",
        "    y=alt.Y(\"gender:N\"),\n",
        "    color=alt.condition(\n",
        "        category_filter,\n",
        "        alt.Color(\"gender:N\", scale=alt.Scale(scheme='category20')),\n",
        "        alt.value(\"lightgray\")),\n",
        ").properties(width=300, height=300, selection=category_filter)\n",
        "\n",
        "# A function that generates a histogram of filtered data.\n",
        "def filtered_hist(field, label, filter):\n",
        "  \"\"\"Creates a layered chart of histograms.\n",
        "  The first layer (light gray) contains the histogram of the full data, and the\n",
        "  second contains the histogram of the filtered data.\n",
        "  Args:\n",
        "    field: the field for which to generate the histogram.\n",
        "    label: String label of the histogram.\n",
        "    filter: an alt.Selection object to be used to filter the data.\n",
        "  \"\"\"\n",
        "  base = alt.Chart().mark_bar().encode(\n",
        "      x=alt.X(field, bin=alt.Bin(maxbins=10), title=label),\n",
        "      y=\"count()\",\n",
        "  ).properties(\n",
        "      width=300,\n",
        "  )\n",
        "  return alt.layer(\n",
        "      base.transform_filter(filter),\n",
        "      base.encode(color=alt.value('lightgray'), opacity=alt.value(.7)),\n",
        "  ).resolve_scale(y='independent')"
      ],
      "metadata": {
        "id": "aF5OB4K6HNd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_ratings = (\n",
        "    ratings\n",
        "    .groupby('user_id', as_index=False)\n",
        "    .agg({'rating': ['count', 'mean']})\n",
        "    .flatten_cols()\n",
        "    .merge(users, on='user_id')\n",
        ")\n",
        "# Create a chart for the count, and one for the mean.\n",
        "alt.hconcat(\n",
        "    filtered_hist('rating count', '# ratings / user', category_filter),\n",
        "    filtered_hist('rating mean', 'mean user rating', category_filter),\n",
        "    category_chart,\n",
        "    data=users_ratings)"
      ],
      "metadata": {
        "id": "T6mIHp7S2o-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_ratings\n"
      ],
      "metadata": {
        "id": "GOF74xyWshff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rating_sparse_tensor(ratings_df):\n",
        "\n",
        "  indices = ratings_df[['user_id', 'item_id']].values\n",
        "  values = ratings_df['rating'].values\n",
        "  return tf.SparseTensor(\n",
        "      indices=indices,\n",
        "      values=values,\n",
        "      dense_shape=[users.shape[0], items.shape[0]])"
      ],
      "metadata": {
        "id": "BP22mz8riOJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_mean_square_error(sparse_ratings, user_embeddings, item_embeddings):\n",
        "\n",
        "  predictions = tf.gather_nd(\n",
        "      tf.matmul(user_embeddings, item_embeddings, transpose_b=True),\n",
        "      sparse_ratings.indices)\n",
        "  loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "YYZWekTdq1P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CFModel helper class (run this cell)\n",
        "class CFModel(object):\n",
        "  \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
        "  def __init__(self, embedding_vars, loss, metrics=None):\n",
        "    \n",
        "    self._embedding_vars = embedding_vars\n",
        "    self._loss = loss\n",
        "    self._metrics = metrics\n",
        "    self._embeddings = {k: None for k in embedding_vars}\n",
        "    self._session = None\n",
        "    self.metrics_v = None\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    \"\"\"The embeddings dictionary.\"\"\"\n",
        "    return self._embeddings\n",
        "\n",
        "  def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
        "            optimizer=tf.train.GradientDescentOptimizer):\n",
        "    \"\"\"Trains the model.\n",
        "    Args:\n",
        "      iterations: number of iterations to run.\n",
        "      learning_rate: optimizer learning rate.\n",
        "      plot_results: whether to plot the results at the end of training.\n",
        "      optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
        "    Returns:\n",
        "      The metrics dictionary evaluated at the last iteration.\n",
        "    \"\"\"\n",
        "    with self._loss.graph.as_default():\n",
        "      opt = optimizer(learning_rate)\n",
        "      train_op = opt.minimize(self._loss)\n",
        "      local_init_op = tf.group(\n",
        "          tf.variables_initializer(opt.variables()),\n",
        "          tf.local_variables_initializer())\n",
        "      if self._session is None:\n",
        "        self._session = tf.Session()\n",
        "        with self._session.as_default():\n",
        "          self._session.run(tf.global_variables_initializer())\n",
        "          self._session.run(tf.tables_initializer())\n",
        "          tf.train.start_queue_runners()\n",
        "\n",
        "    with self._session.as_default():\n",
        "      local_init_op.run()\n",
        "      iterations = []\n",
        "      metrics = self._metrics or ({},)\n",
        "      metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
        "\n",
        "      # Train and append results.\n",
        "      for i in range(num_iterations + 1):\n",
        "        _, results = self._session.run((train_op, metrics))\n",
        "        if (i % 10 == 0) or i == num_iterations:\n",
        "          print(\"\\r iteration %d: \" % i + \", \".join(\n",
        "                [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
        "                end='')\n",
        "          iterations.append(i)\n",
        "          for metric_val, result in zip(metrics_vals, results):\n",
        "            for k, v in result.items():\n",
        "              metric_val[k].append(v)\n",
        "\n",
        "      for k, v in self._embedding_vars.items():\n",
        "        self._embeddings[k] = v.eval()\n",
        "\n",
        "      if plot_results:\n",
        "        # Plot the metrics.\n",
        "        num_subplots = len(metrics)+1\n",
        "        self.metrics_v = metrics_vals\n",
        "        fig = plt.figure()\n",
        "        fig.set_size_inches(num_subplots*10, 8)\n",
        "        for i, metric_vals in enumerate(metrics_vals):\n",
        "          ax = fig.add_subplot(1, num_subplots, i+1)\n",
        "          for k, v in metric_vals.items():\n",
        "            ax.plot(iterations, v, label=k)\n",
        "          ax.set_xlim([1, num_iterations])\n",
        "          ax.legend()\n",
        "      return results"
      ],
      "metadata": {
        "id": "ZRMMd4fb_mDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    ratings: a DataFrame of the ratings\n",
        "    embedding_dim: the dimension of the embedding vectors.\n",
        "    init_stddev: float, the standard deviation of the random initial embeddings.\n",
        "  Returns:\n",
        "    model: a CFModel.\n",
        "  \"\"\"\n",
        "  # Split the ratings DataFrame into train and test.\n",
        "  train_ratings, test_ratings = split_dataframe(ratings)\n",
        "  # SparseTensor representation of the train and test datasets.\n",
        "  A_train = build_rating_sparse_tensor(train_ratings)\n",
        "  A_test = build_rating_sparse_tensor(test_ratings)\n",
        "  # Initialize the embeddings using a normal distribution.\n",
        "  U = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
        "  V = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
        "  train_loss = sparse_mean_square_error(A_train, U, V)\n",
        "  test_loss = sparse_mean_square_error(A_test, U, V)\n",
        "  print(train_loss)\n",
        "  print(test_loss)\n",
        "  print(U)\n",
        "  print(V)  \n",
        "  metrics = {\n",
        "      'train_error': train_loss,\n",
        "      'test_error': test_loss\n",
        "  }\n",
        "  embeddings = {\n",
        "      \"user_id\": U,\n",
        "      \"item_id\": V\n",
        "  }\n",
        "  return CFModel(embeddings, train_loss, [metrics])"
      ],
      "metadata": {
        "id": "4dBSOo9r_6OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(ratings, embedding_dim=100, init_stddev=0.5)\n",
        "model.train(num_iterations=10, learning_rate=10)"
      ],
      "metadata": {
        "id": "17rZOcHQARga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DOT = 'dot'\n",
        "COSINE = 'cosine'\n",
        "def compute_scores(query_embedding, item_embeddings, measure=DOT):\n",
        "  \"\"\"Computes the scores of the candidates given a query.\n",
        "  Args:\n",
        "    query_embedding: a vector of shape [k], representing the query embedding.\n",
        "    item_embeddings: a matrix of shape [N, k], such that row i is the embedding\n",
        "      of item i.\n",
        "    measure: a string specifying the similarity measure to be used. Can be\n",
        "      either DOT or COSINE.\n",
        "  Returns:\n",
        "    scores: a vector of shape [N], such that scores[i] is the score of item i.\n",
        "  \"\"\"\n",
        "  u = query_embedding\n",
        "  V = item_embeddings\n",
        "  if measure == COSINE:\n",
        "    V = V / np.linalg.norm(V, axis=1, keepdims=True)\n",
        "    u = u / np.linalg.norm(u)\n",
        "  scores = u.dot(V.T)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "q_ZIFXRES7PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title User recommendations and nearest neighbors (run this cell)\n",
        "def user_recommendations(model, measure=DOT, exclude_rated=False, k=6):\n",
        "\n",
        "    scores = compute_scores(\n",
        "        model.embeddings[\"user_id\"], model.embeddings[\"item_id\"], measure)\n",
        "    score_key = measure + ' score'\n",
        "    df = pd.DataFrame({\n",
        "        score_key: list(scores),\n",
        "        'item_id': items['item_id'],\n",
        "        'categories': items['category']\n",
        "    })\n",
        "    if exclude_rated:\n",
        "      # remove items that are already rated\n",
        "      rated_items = ratings[ratings.user_id == \"1888\"][\"item_id\"].values\n",
        "      df = df[df.item_id.apply(lambda item_id: item_id not in rated_items)]\n",
        "    display.display(df.sort_values([score_key], ascending=False).head(k))  \n",
        "\n",
        "def movie_neighbors(model, title_substring, measure=DOT, k=6):\n",
        "  # Search for movie ids that match the given substring.\n",
        "  ids =  items[items['category'].str.contains(title_substring)].index.values\n",
        "  categories = items.iloc[ids]['category'].values\n",
        "  if len(categories) == 0:\n",
        "    raise ValueError(\"Found no movies with title %s\" % title_substring)\n",
        "  print(\"Nearest neighbors of : %s.\" % categories[0])\n",
        "  if len(categories) > 1:\n",
        "    print(\"[Found more than one matching item. Other candidates: {}]\".format(\n",
        "        \", \".join(categories[1:])))\n",
        "  item_id = ids[0]\n",
        "  scores = compute_scores(\n",
        "      model.embeddings[\"item_id\"][item_id], model.embeddings[\"item_id\"],\n",
        "      measure)\n",
        "  score_key = measure + ' score'\n",
        "  df = pd.DataFrame({\n",
        "      score_key: list(scores),\n",
        "      'categories': items['category']\n",
        "  })\n",
        "  display.display(df.sort_values([score_key], ascending=False).head(k))"
      ],
      "metadata": {
        "id": "H3XuIy-N3s26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_recommendations(model, measure=COSINE, k=15)"
      ],
      "metadata": {
        "id": "0ILF6ObByFc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eyb7vhW9cOAr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Recommendations System",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}